{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba5de462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T06:21:08.080869Z",
     "iopub.status.busy": "2024-11-02T06:21:08.080396Z",
     "iopub.status.idle": "2024-11-02T06:23:33.444378Z",
     "shell.execute_reply": "2024-11-02T06:23:33.442884Z"
    },
    "id": "_1DO8Wx09ltG",
    "outputId": "a1bfb08a-4b26-4b7c-99ce-0b07c2a62cbe",
    "papermill": {
     "duration": 145.377729,
     "end_time": "2024-11-02T06:23:33.447194",
     "exception": false,
     "start_time": "2024-11-02T06:21:08.069465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.4.0+cpu)\r\n",
      "Requirement already satisfied: nibabel in /opt/conda/lib/python3.10/site-packages (5.2.1)\r\n",
      "Requirement already satisfied: torchinfo in /opt/conda/lib/python3.10/site-packages (1.8.0)\r\n",
      "Collecting monai[all]\r\n",
      "  Downloading monai-1.4.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2024.6.1)\r\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.10/site-packages (from nibabel) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=17 in /opt/conda/lib/python3.10/site-packages (from nibabel) (21.3)\r\n",
      "Collecting clearml>=1.10.0rc0 (from monai[all])\r\n",
      "  Downloading clearml-1.16.5-py2.py3-none-any.whl.metadata (17 kB)\r\n",
      "Collecting einops (from monai[all])\r\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting fire (from monai[all])\r\n",
      "  Downloading fire-0.7.0.tar.gz (87 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting gdown>=4.7.3 (from monai[all])\r\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from monai[all]) (3.11.0)\r\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from monai[all]) (0.25.1)\r\n",
      "Collecting imagecodecs (from monai[all])\r\n",
      "  Downloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\r\n",
      "Collecting itk>=5.2 (from monai[all])\r\n",
      "  Downloading itk-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (22 kB)\r\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from monai[all]) (4.22.0)\r\n",
      "Collecting lmdb (from monai[all])\r\n",
      "  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\r\n",
      "Collecting lpips==0.1.4 (from monai[all])\r\n",
      "  Downloading lpips-0.1.4-py3-none-any.whl.metadata (10 kB)\r\n",
      "Requirement already satisfied: matplotlib>=3.6.3 in /opt/conda/lib/python3.10/site-packages (from monai[all]) (3.7.5)\r\n",
      "Collecting mlflow>=2.12.2 (from monai[all])\r\n",
      "  Downloading mlflow-2.17.2-py3-none-any.whl.metadata (29 kB)\r\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from monai[all]) (1.11.1.1)\r\n",
      "Collecting nni (from monai[all])\r\n",
      "  Downloading nni-3.0-py3-none-manylinux1_x86_64.whl.metadata (19 kB)\r\n",
      "Requirement already satisfied: nvidia-ml-py in /opt/conda/lib/python3.10/site-packages (from monai[all]) (11.495.46)\r\n",
      "Requirement already satisfied: onnx>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from monai[all]) (1.17.0)\r\n",
      "Requirement already satisfied: openslide-python in /opt/conda/lib/python3.10/site-packages (from monai[all]) (1.3.1)\r\n",
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.10/site-packages (from monai[all]) (4.0.0)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from monai[all]) (2.2.3)\r\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.10/site-packages (from monai[all]) (10.3.0)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from monai[all]) (5.9.3)\r\n",
      "Collecting pyamg>=5.0.0 (from monai[all])\r\n",
      "  Downloading pyamg-5.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\r\n",
      "Requirement already satisfied: pydicom in /opt/conda/lib/python3.10/site-packages (from monai[all]) (3.0.1)\r\n",
      "Collecting pynrrd (from monai[all])\r\n",
      "  Downloading pynrrd-1.0.0-py2.py3-none-any.whl.metadata (3.9 kB)\r\n",
      "Collecting pytorch-ignite==0.4.11 (from monai[all])\r\n",
      "  Downloading pytorch_ignite-0.4.11-py3-none-any.whl.metadata (28 kB)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from monai[all]) (6.0.2)\r\n",
      "Requirement already satisfied: scikit-image>=0.14.2 in /opt/conda/lib/python3.10/site-packages (from monai[all]) (0.23.2)\r\n",
      "Requirement already satisfied: tensorboard in /opt/conda/lib/python3.10/site-packages (from monai[all]) (2.16.2)\r\n",
      "Requirement already satisfied: tensorboardX in /opt/conda/lib/python3.10/site-packages (from monai[all]) (2.6.2.2)\r\n",
      "Requirement already satisfied: tifffile in /opt/conda/lib/python3.10/site-packages (from monai[all]) (2024.5.22)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from monai[all]) (0.19.0+cpu)\r\n",
      "Requirement already satisfied: tqdm>=4.47.0 in /opt/conda/lib/python3.10/site-packages (from monai[all]) (4.66.4)\r\n",
      "Collecting zarr (from monai[all])\r\n",
      "  Downloading zarr-2.18.3-py3-none-any.whl.metadata (5.7 kB)\r\n",
      "Collecting onnxruntime (from monai[all])\r\n",
      "  Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\r\n",
      "Collecting transformers<4.41.0,>=4.36.0 (from monai[all])\r\n",
      "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from monai[all]) (1.14.1)\r\n",
      "Collecting cucim-cu12 (from monai[all])\r\n",
      "  Downloading cucim_cu12-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (7.5 kB)\r\n",
      "Requirement already satisfied: attrs>=18.0 in /opt/conda/lib/python3.10/site-packages (from clearml>=1.10.0rc0->monai[all]) (23.2.0)\r\n",
      "Collecting furl>=2.0.0 (from clearml>=1.10.0rc0->monai[all])\r\n",
      "  Downloading furl-2.1.3-py2.py3-none-any.whl.metadata (1.2 kB)\r\n",
      "Collecting pathlib2>=2.3.0 (from clearml>=1.10.0rc0->monai[all])\r\n",
      "  Downloading pathlib2-2.3.7.post1-py2.py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in /opt/conda/lib/python3.10/site-packages (from clearml>=1.10.0rc0->monai[all]) (3.1.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.10/site-packages (from clearml>=1.10.0rc0->monai[all]) (2.9.0.post0)\r\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from clearml>=1.10.0rc0->monai[all]) (2.32.3)\r\n",
      "Requirement already satisfied: six>=1.16.0 in /opt/conda/lib/python3.10/site-packages (from clearml>=1.10.0rc0->monai[all]) (1.16.0)\r\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from clearml>=1.10.0rc0->monai[all]) (1.26.18)\r\n",
      "Requirement already satisfied: pyjwt<2.9.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from clearml>=1.10.0rc0->monai[all]) (2.8.0)\r\n",
      "Requirement already satisfied: referencing<0.40 in /opt/conda/lib/python3.10/site-packages (from clearml>=1.10.0rc0->monai[all]) (0.35.1)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown>=4.7.3->monai[all]) (4.12.3)\r\n",
      "Collecting itk-core==5.4.0 (from itk>=5.2->monai[all])\r\n",
      "  Downloading itk_core-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (22 kB)\r\n",
      "Collecting itk-numerics==5.4.0 (from itk>=5.2->monai[all])\r\n",
      "  Downloading itk_numerics-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (22 kB)\r\n",
      "Collecting itk-io==5.4.0 (from itk>=5.2->monai[all])\r\n",
      "  Downloading itk_io-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (22 kB)\r\n",
      "Collecting itk-filtering==5.4.0 (from itk>=5.2->monai[all])\r\n",
      "  Downloading itk_filtering-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (22 kB)\r\n",
      "Collecting itk-registration==5.4.0 (from itk>=5.2->monai[all])\r\n",
      "  Downloading itk_registration-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (22 kB)\r\n",
      "Collecting itk-segmentation==5.4.0 (from itk>=5.2->monai[all])\r\n",
      "  Downloading itk_segmentation-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (22 kB)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->monai[all]) (2023.12.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->monai[all]) (0.18.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.3->monai[all]) (1.2.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.3->monai[all]) (0.12.1)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.3->monai[all]) (4.53.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.6.3->monai[all]) (1.4.5)\r\n",
      "Collecting mlflow-skinny==2.17.2 (from mlflow>=2.12.2->monai[all])\r\n",
      "  Downloading mlflow_skinny-2.17.2-py3-none-any.whl.metadata (30 kB)\r\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.12.2->monai[all]) (3.0.3)\r\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.12.2->monai[all]) (1.13.3)\r\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.12.2->monai[all]) (7.1.0)\r\n",
      "Collecting graphene<4 (from mlflow>=2.12.2->monai[all])\r\n",
      "  Downloading graphene-3.4.1-py2.py3-none-any.whl.metadata (6.9 kB)\r\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.12.2->monai[all]) (3.6)\r\n",
      "Requirement already satisfied: pyarrow<18,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.12.2->monai[all]) (17.0.0)\r\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.12.2->monai[all]) (1.2.2)\r\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow>=2.12.2->monai[all]) (2.0.30)\r\n",
      "Collecting gunicorn<24 (from mlflow>=2.12.2->monai[all])\r\n",
      "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\r\n",
      "Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all])\r\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\r\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (8.1.7)\r\n",
      "Requirement already satisfied: cloudpickle<4 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (3.0.0)\r\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all])\r\n",
      "  Downloading databricks_sdk-0.36.0-py3-none-any.whl.metadata (38 kB)\r\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (3.1.43)\r\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (7.0.0)\r\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (1.25.0)\r\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (1.25.0)\r\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (3.20.3)\r\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (0.5.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->monai[all]) (2024.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->monai[all]) (2024.1)\r\n",
      "Requirement already satisfied: imageio>=2.33 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->monai[all]) (2.34.1)\r\n",
      "Requirement already satisfied: lazy-loader>=0.4 in /opt/conda/lib/python3.10/site-packages (from scikit-image>=0.14.2->monai[all]) (0.4)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.36.0->monai[all]) (2024.5.15)\r\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<4.41.0,>=4.36.0->monai[all])\r\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<4.41.0,>=4.36.0->monai[all]) (0.4.5)\r\n",
      "Collecting cupy-cuda12x>=12.0.0 (from cucim-cu12->monai[all])\r\n",
      "  Downloading cupy_cuda12x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.7 kB)\r\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.10/site-packages (from fire->monai[all]) (2.4.0)\r\n",
      "Collecting astor (from nni->monai[all])\r\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl.metadata (4.2 kB)\r\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from nni->monai[all]) (0.4.6)\r\n",
      "Collecting filelock (from torch)\r\n",
      "  Downloading filelock-3.11.0-py3-none-any.whl.metadata (2.5 kB)\r\n",
      "Collecting json-tricks>=3.15.5 (from nni->monai[all])\r\n",
      "  Downloading json_tricks-3.17.3-py2.py3-none-any.whl.metadata (16 kB)\r\n",
      "Requirement already satisfied: prettytable in /opt/conda/lib/python3.10/site-packages (from nni->monai[all]) (3.10.0)\r\n",
      "Collecting PythonWebHDFS (from nni->monai[all])\r\n",
      "  Downloading PythonWebHDFS-0.2.3-py3-none-any.whl.metadata (717 bytes)\r\n",
      "Collecting responses (from nni->monai[all])\r\n",
      "  Downloading responses-0.25.3-py3-none-any.whl.metadata (46 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting schema (from nni->monai[all])\r\n",
      "  Downloading schema-0.7.7-py2.py3-none-any.whl.metadata (34 kB)\r\n",
      "Collecting typeguard<4.1.3,>=3.0.0 (from nni->monai[all])\r\n",
      "  Downloading typeguard-4.1.2-py3-none-any.whl.metadata (3.7 kB)\r\n",
      "Requirement already satisfied: websockets>=10.1 in /opt/conda/lib/python3.10/site-packages (from nni->monai[all]) (12.0)\r\n",
      "Collecting coloredlogs (from onnxruntime->monai[all])\r\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime->monai[all]) (24.3.25)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.10/site-packages (from optuna->monai[all]) (6.8.2)\r\n",
      "Collecting nptyping (from pynrrd->monai[all])\r\n",
      "  Downloading nptyping-2.5.0-py3-none-any.whl.metadata (7.6 kB)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard->monai[all]) (1.4.0)\r\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard->monai[all]) (1.64.1)\r\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->monai[all]) (70.0.0)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard->monai[all]) (0.7.2)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard->monai[all]) (3.0.4)\r\n",
      "Collecting asciitree (from zarr->monai[all])\r\n",
      "  Downloading asciitree-0.3.3.tar.gz (4.0 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hCollecting numcodecs>=0.10.0 (from zarr->monai[all])\r\n",
      "  Downloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\r\n",
      "Requirement already satisfied: fasteners in /opt/conda/lib/python3.10/site-packages (from zarr->monai[all]) (0.19)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.10/site-packages (from alembic!=1.10.0,<2->mlflow>=2.12.2->monai[all]) (1.3.5)\r\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x>=12.0.0->cucim-cu12->monai[all])\r\n",
      "  Downloading fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (9.3 kB)\r\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow>=2.12.2->monai[all]) (2.2.0)\r\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.10/site-packages (from Flask<4->mlflow>=2.12.2->monai[all]) (1.8.2)\r\n",
      "Collecting orderedmultidict>=1.0.1 (from furl>=2.0.0->clearml>=1.10.0rc0->monai[all])\r\n",
      "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl.metadata (1.3 kB)\r\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow>=2.12.2->monai[all])\r\n",
      "  Downloading graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\r\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow>=2.12.2->monai[all])\r\n",
      "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml>=1.10.0rc0->monai[all]) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml>=1.10.0rc0->monai[all]) (3.7)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->clearml>=1.10.0rc0->monai[all]) (2024.8.30)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.12.2->monai[all]) (1.4.2)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn<2->mlflow>=2.12.2->monai[all]) (3.5.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.12.2->monai[all]) (3.0.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown>=4.7.3->monai[all]) (2.5)\r\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->monai[all])\r\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\r\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from prettytable->nni->monai[all]) (0.2.13)\r\n",
      "Collecting simplejson (from PythonWebHDFS->nni->monai[all])\r\n",
      "  Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\r\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown>=4.7.3->monai[all]) (1.7.1)\r\n",
      "Requirement already satisfied: google-auth~=2.0 in /opt/conda/lib/python3.10/site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (2.30.0)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (4.0.11)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (3.19.2)\r\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (1.2.14)\r\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.46b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (0.46b0)\r\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.10/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (1.16.0)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (5.0.1)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (0.4.0)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (4.9)\r\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.17.2->mlflow>=2.12.2->monai[all]) (0.6.0)\r\n",
      "Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pytorch_ignite-0.4.11-py3-none-any.whl (266 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.5/266.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading clearml-1.16.5-py2.py3-none-any.whl (1.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\r\n",
      "Downloading itk-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (17 kB)\r\n",
      "Downloading itk_core-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (80.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.2/80.2 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading itk_filtering-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (67.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading itk_io-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (28.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.0/28.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading itk_numerics-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (57.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading itk_registration-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (28.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.4/28.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading itk_segmentation-5.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (15.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.8/15.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mlflow-2.17.2-py3-none-any.whl (26.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading mlflow_skinny-2.17.2-py3-none-any.whl (5.7 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyamg-5.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cucim_cu12-24.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading einops-0.8.0-py3-none-any.whl (43 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading imagecodecs-2024.9.22-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (294 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading monai-1.4.0-py3-none-any.whl (1.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nni-3.0-py3-none-manylinux1_x86_64.whl (61.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/61.4 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading filelock-3.11.0-py3-none-any.whl (10.0 kB)\r\n",
      "Downloading onnxruntime-1.20.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pynrrd-1.0.0-py2.py3-none-any.whl (19 kB)\r\n",
      "Downloading zarr-2.18.3-py3-none-any.whl (210 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading cupy_cuda12x-13.3.0-cp310-cp310-manylinux2014_x86_64.whl (90.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\r\n",
      "Downloading graphene-3.4.1-py2.py3-none-any.whl (114 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.7/114.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading json_tricks-3.17.3-py2.py3-none-any.whl (27 kB)\r\n",
      "Downloading numcodecs-0.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pathlib2-2.3.7.post1-py2.py3-none-any.whl (18 kB)\r\n",
      "Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading typeguard-4.1.2-py3-none-any.whl (33 kB)\r\n",
      "Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\r\n",
      "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nptyping-2.5.0-py3-none-any.whl (37 kB)\r\n",
      "Downloading PythonWebHDFS-0.2.3-py3-none-any.whl (10 kB)\r\n",
      "Downloading responses-0.25.3-py3-none-any.whl (55 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.2/55.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading schema-0.7.7-py2.py3-none-any.whl (18 kB)\r\n",
      "Downloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\r\n",
      "Downloading databricks_sdk-0.36.0-py3-none-any.whl (569 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.1/569.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading fastrlock-0.8.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (51 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.3/51.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_core-3.2.5-py3-none-any.whl (203 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\r\n",
      "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\r\n",
      "Downloading simplejson-3.19.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: fire, asciitree\r\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114248 sha256=edbb0698e07208a1c51afaf2e9f1f0e82d5bc0b04d74e202fdf47b4ecad25ffa\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\r\n",
      "  Building wheel for asciitree (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for asciitree: filename=asciitree-0.3.3-py3-none-any.whl size=5034 sha256=395473453db4a0a0cfad9711fcc7238b495d4a6fa5ed5fe0353cc668775bf8c1\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/7f/4e/be/1171b40f43b918087657ec57cf3b81fa1a2e027d8755baa184\r\n",
      "Successfully built fire asciitree\r\n",
      "Installing collected packages: schema, lmdb, json-tricks, fastrlock, asciitree, typeguard, simplejson, pathlib2, orderedmultidict, numcodecs, nptyping, itk-core, imagecodecs, humanfriendly, graphql-core, fire, filelock, einops, cupy-cuda12x, cachetools, astor, zarr, responses, PythonWebHDFS, pynrrd, pyamg, itk-numerics, itk-io, gunicorn, graphql-relay, furl, coloredlogs, tokenizers, pytorch-ignite, onnxruntime, nni, monai, itk-filtering, graphene, gdown, databricks-sdk, transformers, lpips, itk-segmentation, itk-registration, cucim-cu12, clearml, mlflow-skinny, itk, mlflow\r\n",
      "  Attempting uninstall: typeguard\r\n",
      "    Found existing installation: typeguard 4.3.0\r\n",
      "    Uninstalling typeguard-4.3.0:\r\n",
      "      Successfully uninstalled typeguard-4.3.0\r\n",
      "  Attempting uninstall: filelock\r\n",
      "    Found existing installation: filelock 3.15.1\r\n",
      "    Uninstalling filelock-3.15.1:\r\n",
      "      Successfully uninstalled filelock-3.15.1\r\n",
      "  Attempting uninstall: cachetools\r\n",
      "    Found existing installation: cachetools 4.2.4\r\n",
      "    Uninstalling cachetools-4.2.4:\r\n",
      "      Successfully uninstalled cachetools-4.2.4\r\n",
      "  Attempting uninstall: tokenizers\r\n",
      "    Found existing installation: tokenizers 0.20.0\r\n",
      "    Uninstalling tokenizers-0.20.0:\r\n",
      "      Successfully uninstalled tokenizers-0.20.0\r\n",
      "  Attempting uninstall: pytorch-ignite\r\n",
      "    Found existing installation: pytorch-ignite 0.5.1\r\n",
      "    Uninstalling pytorch-ignite-0.5.1:\r\n",
      "      Successfully uninstalled pytorch-ignite-0.5.1\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.45.1\r\n",
      "    Uninstalling transformers-4.45.1:\r\n",
      "      Successfully uninstalled transformers-4.45.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "dataproc-jupyter-plugin 0.1.79 requires pydantic~=1.10.0, but you have pydantic 2.9.2 which is incompatible.\r\n",
      "ibis-framework 7.1.0 requires pyarrow<15,>=2, but you have pyarrow 17.0.0 which is incompatible.\r\n",
      "kfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\r\n",
      "ydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed PythonWebHDFS-0.2.3 asciitree-0.3.3 astor-0.8.1 cachetools-5.3.3 clearml-1.16.5 coloredlogs-15.0.1 cucim-cu12-24.10.0 cupy-cuda12x-13.3.0 databricks-sdk-0.36.0 einops-0.8.0 fastrlock-0.8.2 filelock-3.11.0 fire-0.7.0 furl-2.1.3 gdown-5.2.0 graphene-3.4.1 graphql-core-3.2.5 graphql-relay-3.2.0 gunicorn-23.0.0 humanfriendly-10.0 imagecodecs-2024.9.22 itk-5.4.0 itk-core-5.4.0 itk-filtering-5.4.0 itk-io-5.4.0 itk-numerics-5.4.0 itk-registration-5.4.0 itk-segmentation-5.4.0 json-tricks-3.17.3 lmdb-1.5.1 lpips-0.1.4 mlflow-2.17.2 mlflow-skinny-2.17.2 monai-1.4.0 nni-3.0 nptyping-2.5.0 numcodecs-0.13.1 onnxruntime-1.20.0 orderedmultidict-1.0.1 pathlib2-2.3.7.post1 pyamg-5.2.1 pynrrd-1.0.0 pytorch-ignite-0.4.11 responses-0.25.3 schema-0.7.7 simplejson-3.19.3 tokenizers-0.19.1 transformers-4.40.2 typeguard-4.1.2 zarr-2.18.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torch nibabel torchinfo 'monai[all]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "520b0b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T06:23:33.643001Z",
     "iopub.status.busy": "2024-11-02T06:23:33.641868Z",
     "iopub.status.idle": "2024-11-02T06:23:49.260931Z",
     "shell.execute_reply": "2024-11-02T06:23:49.258075Z"
    },
    "id": "gVCMyXtX3yvx",
    "outputId": "07c16db6-a756-48c5-cdbe-c2b276d7cda4",
    "papermill": {
     "duration": 15.724835,
     "end_time": "2024-11-02T06:23:49.268651",
     "exception": false,
     "start_time": "2024-11-02T06:23:33.543816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /opt/conda/lib/python3.10/site-packages (1.4.2)\r\n",
      "Requirement already satisfied: numpy>1.20.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>17.1 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (21.3)\r\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (2.4.0+cpu)\r\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from torchmetrics) (0.11.7)\r\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (70.0.0)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>17.1->torchmetrics) (3.1.2)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.11.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (1.12)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->torchmetrics) (2024.6.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchmetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64aa934b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T06:23:49.472833Z",
     "iopub.status.busy": "2024-11-02T06:23:49.472200Z",
     "iopub.status.idle": "2024-11-02T06:25:11.250875Z",
     "shell.execute_reply": "2024-11-02T06:25:11.249412Z"
    },
    "id": "QvTDWu7Eut0b",
    "papermill": {
     "duration": 81.883667,
     "end_time": "2024-11-02T06:25:11.254629",
     "exception": false,
     "start_time": "2024-11-02T06:23:49.370962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from monai.networks.nets import AttentionUnet\n",
    "from monai.networks.layers import Norm\n",
    "import torchinfo\n",
    "\n",
    "import zipfile\n",
    "import random\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from torchmetrics.functional import structural_similarity_index_measure\n",
    "from monai.metrics import DiceMetric\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ed8d6d",
   "metadata": {
    "id": "PZemn3piugZ8",
    "papermill": {
     "duration": 0.109675,
     "end_time": "2024-11-02T06:25:11.471067",
     "exception": false,
     "start_time": "2024-11-02T06:25:11.361392",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> # **Model : Attention Unet**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad0fd9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-02T06:25:11.683241Z",
     "iopub.status.busy": "2024-11-02T06:25:11.682028Z",
     "iopub.status.idle": "2024-11-02T06:25:12.421040Z",
     "shell.execute_reply": "2024-11-02T06:25:12.419215Z"
    },
    "id": "_wjTVHeUZFEI",
    "papermill": {
     "duration": 0.851157,
     "end_time": "2024-11-02T06:25:12.423462",
     "exception": true,
     "start_time": "2024-11-02T06:25:11.572305",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CustomAttentionUnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomAttentionUnet\u001b[49m(\n\u001b[1;32m      4\u001b[0m     spatial_dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      5\u001b[0m     in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      6\u001b[0m     out_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     channels\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m),\n\u001b[1;32m      8\u001b[0m     strides\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m      9\u001b[0m     kernel_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CustomAttentionUnet' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CustomAttentionUnet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    kernel_size=3\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa995d2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T06:58:05.326866Z",
     "iopub.status.busy": "2024-10-28T06:58:05.326558Z",
     "iopub.status.idle": "2024-10-28T06:58:06.685590Z",
     "shell.execute_reply": "2024-10-28T06:58:06.684629Z",
     "shell.execute_reply.started": "2024-10-28T06:58:05.326834Z"
    },
    "id": "v7mumobvZW8d",
    "outputId": "f8ad2330-3596-45b8-8b53-23f8ca7b625a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torchinfo.summary(model, input_size=(8, 1, 16,  128,  128))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd50c5",
   "metadata": {
    "id": "7FQXQtMn8-MF",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "> # Dataset : openBHB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f8bfc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T06:58:07.631680Z",
     "iopub.status.busy": "2024-10-28T06:58:07.631372Z",
     "iopub.status.idle": "2024-10-28T06:58:07.647145Z",
     "shell.execute_reply": "2024-10-28T06:58:07.646255Z",
     "shell.execute_reply.started": "2024-10-28T06:58:07.631647Z"
    },
    "id": "LR-wbRrYzgsm",
    "outputId": "4f1b248c-3cb6-4a8c-fd20-967d91c9782a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_data(source_dir, test_ratio=0.2, max_volumes=500):\n",
    "    # Get a list of all volume files in the source directory\n",
    "    volumes = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
    "\n",
    "    # Limit to the first 400 volumes\n",
    "    volumes = volumes[:max_volumes]\n",
    "\n",
    "    # Shuffle volumes randomly\n",
    "    random.shuffle(volumes)\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(volumes) * (1 - test_ratio))\n",
    "\n",
    "    # Split volumes into train and test sets\n",
    "    train_volumes = volumes[:split_index]\n",
    "    test_volumes = volumes[split_index:]\n",
    "\n",
    "    return train_volumes, test_volumes\n",
    "\n",
    "# Usage example:\n",
    "source_dir = \"/kaggle/input/openbhb/val_quasiraw\"\n",
    "test_ratio = 0.1  # Adjust this to change the test/train split\n",
    "\n",
    "train_volumes, test_volumes = split_data(source_dir, test_ratio, max_volumes=600)\n",
    "\n",
    "\n",
    "print(\"Training set volumes:\", len(train_volumes))\n",
    "print(\"Testing set volumes:\", len(test_volumes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4190b3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T06:58:07.648635Z",
     "iopub.status.busy": "2024-10-28T06:58:07.648311Z",
     "iopub.status.idle": "2024-10-28T06:58:07.663777Z",
     "shell.execute_reply": "2024-10-28T06:58:07.662820Z",
     "shell.execute_reply.started": "2024-10-28T06:58:07.648604Z"
    },
    "id": "-YIDSOOG9rOa",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class BrainMRIDataset(Dataset):\n",
    "    def __init__(self, base_path, volume_list, target_size=(88, 128, 128), slice_depth=16, transform=None):\n",
    "        self.base_path = base_path\n",
    "        self.volume_list = volume_list\n",
    "        self.target_size = target_size\n",
    "        self.slice_depth = slice_depth\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.volume_list)\n",
    "\n",
    "    def crop(self, volume, start_y=20, end_y=160, start_x=20, end_x=196, start_z=50, end_z=130):\n",
    "        cropped_volume = volume[start_z:end_z, start_y:end_y, start_x:end_x]\n",
    "        return cropped_volume\n",
    "\n",
    "    def get_slices(self, mri_volume):\n",
    "        if len(mri_volume.shape) != 3:\n",
    "            mri_volume = mri_volume.squeeze()\n",
    "\n",
    "        patches = []\n",
    "        num_slices = mri_volume.shape[0] // self.slice_depth\n",
    "\n",
    "        for i in range(num_slices):\n",
    "            start = i * self.slice_depth\n",
    "            end = start + self.slice_depth\n",
    "            patch = mri_volume[start:end, :, :]\n",
    "            patches.append(patch)\n",
    "\n",
    "        remainder = mri_volume.shape[0] % self.slice_depth\n",
    "        if remainder > 0:\n",
    "            patch = mri_volume[-self.slice_depth:, :, :]\n",
    "            patches.append(patch)\n",
    "\n",
    "        return patches\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the filename for the current volume\n",
    "        volume_file = self.volume_list[index]\n",
    "\n",
    "        # Construct the file path\n",
    "        file_path = os.path.join(self.base_path, volume_file)\n",
    "\n",
    "        # Load the .npy file\n",
    "        mri_volume = np.load(file_path)  # Shape: (1, 1, 182, 218, 182)\n",
    "\n",
    "        # Convert to tensor and remove redundant dimensions\n",
    "        mri_volume = torch.tensor(mri_volume).float().squeeze().squeeze()  # Shape: [182, 218, 182]\n",
    "\n",
    "        # (C, H, W, D ) - > (C, D, H, W)\n",
    "        mri_volume = mri_volume.permute(2, 0, 1)\n",
    "\n",
    "        # Crop to (80, 140, 176)\n",
    "        mri_crop = self.crop(mri_volume)\n",
    "\n",
    "        # Resize to target shape (128x128x128)\n",
    "        mri_resize = F.interpolate(\n",
    "            mri_crop.unsqueeze(0).unsqueeze(0),\n",
    "            size=(mri_crop.shape[0], 128, 128),\n",
    "            mode='trilinear',\n",
    "            align_corners=False\n",
    "        ).squeeze()\n",
    "\n",
    "        # Normalize the volume\n",
    "        mri_volume = (mri_resize - mri_resize.min()) / (mri_resize.max() - mri_resize.min() + 1e-8)\n",
    "\n",
    "        # Slice the volume along the depth axis\n",
    "        mri_slices = self.get_slices(mri_volume)\n",
    "\n",
    "        # Add channel dimension back to each slice\n",
    "        mri_slices = [slice.unsqueeze(0) for slice in mri_slices]  # Shape: [1, 128, 128, 8]\n",
    "\n",
    "        return mri_slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8822ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T06:58:07.668255Z",
     "iopub.status.busy": "2024-10-28T06:58:07.667781Z",
     "iopub.status.idle": "2024-10-28T06:58:08.415113Z",
     "shell.execute_reply": "2024-10-28T06:58:08.414048Z",
     "shell.execute_reply.started": "2024-10-28T06:58:07.668205Z"
    },
    "id": "BY9t-ekM-qMt",
    "outputId": "d18db0a6-7197-4071-8a46-f8b3a192bd4c",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "dataset = BrainMRIDataset(base_path='/kaggle/input/openbhb/val_quasiraw', volume_list=train_volumes)\n",
    "\n",
    "# Test the dataset\n",
    "volume_slices = dataset[0]\n",
    "print(f\"Dataset length : {len(dataset)}, Number of slices: {len(volume_slices)}, Slice shape: {volume_slices[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1597c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T06:58:08.416738Z",
     "iopub.status.busy": "2024-10-28T06:58:08.416419Z",
     "iopub.status.idle": "2024-10-28T07:03:02.432555Z",
     "shell.execute_reply": "2024-10-28T07:03:02.431532Z",
     "shell.execute_reply.started": "2024-10-28T06:58:08.416699Z"
    },
    "id": "_0lYZ4hb-6u0",
    "outputId": "903457ea-6711-447d-9418-9b5c2722c0ff",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SliceDatasetFromList(Dataset):\n",
    "    def __init__(self, patch_list):\n",
    "        self.patch_list = [patch for sublist in patch_list for patch in sublist]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patch_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        patch = self.patch_list[index]\n",
    "        patch_tensor = torch.tensor(patch).float()\n",
    "\n",
    "        return patch_tensor, patch_tensor\n",
    "\n",
    "\n",
    "Slices = SliceDatasetFromList(dataset)\n",
    "\n",
    "train_ratio = 0.85\n",
    "val_ratio = 0.15\n",
    "\n",
    "train_size = int(train_ratio * len(Slices))\n",
    "val_size = len(Slices) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(Slices, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"Train set size:\", len(train_dataset))\n",
    "print(\"Validation set size:\", len(val_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5560f9ec",
   "metadata": {
    "id": "d0QOWwKNCRAx",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "> # **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10630590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T07:08:58.048293Z",
     "iopub.status.busy": "2024-10-28T07:08:58.047308Z",
     "iopub.status.idle": "2024-10-28T07:08:58.055902Z",
     "shell.execute_reply": "2024-10-28T07:08:58.054889Z",
     "shell.execute_reply.started": "2024-10-28T07:08:58.048252Z"
    },
    "id": "rikPmeRN1-ai",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SSIM_MSE_Loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the combined SSIM + MSE loss without weighting.\n",
    "        \"\"\"\n",
    "        super(SSIM_MSE_Loss, self).__init__()\n",
    "\n",
    "    def forward(self, predicted, target):\n",
    "        # Calculate SSIM loss\n",
    "        ssim_loss = 1 - structural_similarity_index_measure(predicted, target, data_range=1.0)\n",
    "\n",
    "        # Calculate MSE loss\n",
    "        mse_loss = F.mse_loss(predicted, target)\n",
    "\n",
    "        # Combined loss (simple addition)\n",
    "        combined_loss = ssim_loss + mse_loss\n",
    "        return combined_loss\n",
    "\n",
    "# Usage:\n",
    "criterion = SSIM_MSE_Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061be5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T07:18:20.932828Z",
     "iopub.status.busy": "2024-10-28T07:18:20.932421Z",
     "iopub.status.idle": "2024-10-28T07:18:20.938808Z",
     "shell.execute_reply": "2024-10-28T07:18:20.937844Z",
     "shell.execute_reply.started": "2024-10-28T07:18:20.932789Z"
    },
    "id": "tTG1pEAtCQX2",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = SSIM_MSE_Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadac44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T13:44:55.416279Z",
     "iopub.status.busy": "2024-10-27T13:44:55.415967Z",
     "iopub.status.idle": "2024-10-27T13:44:55.424005Z",
     "shell.execute_reply": "2024-10-27T13:44:55.422801Z",
     "shell.execute_reply.started": "2024-10-27T13:44:55.416219Z"
    },
    "id": "kGI1jsh5B6BQ",
    "outputId": "c59604b5-5854-4743-a8a2-084ef124350a",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9535389",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T07:18:23.703624Z",
     "iopub.status.busy": "2024-10-28T07:18:23.702681Z",
     "iopub.status.idle": "2024-10-28T07:18:23.710210Z",
     "shell.execute_reply": "2024-10-28T07:18:23.709301Z",
     "shell.execute_reply.started": "2024-10-28T07:18:23.703573Z"
    },
    "id": "OdqtNiiGB8Hf",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, checkpoint_dir=\"/kaggle/working/checkpoint_attUNET\", filename=\"checkpoint_MSE_SSIM.pth\"):\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, filename)\n",
    "    torch.save(state, checkpoint_path)\n",
    "    if is_best:\n",
    "        best_model_path = os.path.join(checkpoint_dir, \"best_model_MSE_SSIM.pth\")\n",
    "        torch.save(state, best_model_path)\n",
    "        print(f\"Best model saved to {best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80922a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T15:42:03.389996Z",
     "iopub.status.busy": "2024-10-27T15:42:03.389586Z",
     "iopub.status.idle": "2024-10-27T20:02:50.047422Z",
     "shell.execute_reply": "2024-10-27T20:02:50.045862Z",
     "shell.execute_reply.started": "2024-10-27T15:42:03.389941Z"
    },
    "id": "8gtnoamtDR0Y",
    "outputId": "3b243530-780b-463c-f5fa-6a7796608023",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import monai\n",
    "from monai.losses import SSIMLoss\n",
    "\n",
    "\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "num_epochs = 50\n",
    "best_val_loss = float('inf')\n",
    "patience = 10\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for imgs, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Training)'):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(imgs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Average training loss for this epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    train_loss_values.append(train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Validation)'):\n",
    "            imgs, targets = imgs.to(device), targets.to(device)\n",
    "            outputs = model(imgs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute validation loss\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "    # Average validation loss for this epoch\n",
    "    val_loss /= len(val_loader)\n",
    "    val_loss_values.append(val_loss)\n",
    "\n",
    "    # Print the losses for this epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    is_best = val_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        # Save the best model checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss\n",
    "        }\n",
    "        save_checkpoint(checkpoint, is_best)  # Ensure save_checkpoint function is defined\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    # Early stopping condition\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}. No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "        break\n",
    "\n",
    "# Print the best validation loss\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(train_loss_values, label='Training Loss')\n",
    "plt.plot(val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea9ed0f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "**Restarting traning from checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a998d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T07:47:02.478144Z",
     "iopub.status.busy": "2024-10-28T07:47:02.477433Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    if os.path.isfile(filepath):\n",
    "        print(f\"Loading checkpoint from '{filepath}'...\")\n",
    "        checkpoint = torch.load(filepath)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_val_loss = checkpoint['val_loss']\n",
    "        train_loss_values = checkpoint.get('train_loss_values', [])\n",
    "        val_loss_values = checkpoint.get('val_loss_values', [])\n",
    "        print(f\"Resuming from epoch {start_epoch} with best validation loss {best_val_loss:.4f}\")\n",
    "        return start_epoch, best_val_loss, train_loss_values, val_loss_values\n",
    "    else:\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 0, float('inf'), [], []\n",
    "\n",
    "\n",
    "# Load the checkpoint (if it exists)\n",
    "checkpoint_path = '/kaggle/input/checkpoint/checkpoint_MSE_SSIM.pth'\n",
    "start_epoch, best_val_loss, train_loss_values, val_loss_values = load_checkpoint(checkpoint_path)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "early_stop_counter = 0\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    # Training loop\n",
    "    for imgs, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Training)'):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(imgs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    # Average training loss for this epoch\n",
    "    train_loss /= len(train_loader)\n",
    "    train_loss_values.append(train_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} (Validation)'):\n",
    "            imgs, targets = imgs.to(device), targets.to(device)\n",
    "            outputs = model(imgs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute validation loss\n",
    "            val_loss += loss.item()  # Accumulate validation loss\n",
    "\n",
    "    # Average validation loss for this epoch\n",
    "    val_loss /= len(val_loader)\n",
    "    val_loss_values.append(val_loss)\n",
    "\n",
    "    # Print the losses for this epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Check if the current validation loss is the best we've seen\n",
    "    is_best = val_loss < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = val_loss\n",
    "        early_stop_counter = 0\n",
    "\n",
    "        # Save the best model checkpoint\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_loss_values': train_loss_values,\n",
    "            'val_loss_values': val_loss_values\n",
    "        }\n",
    "        save_checkpoint(checkpoint, is_best)  # Save checkpoint with current best model\n",
    "    else:\n",
    "        early_stop_counter += 1\n",
    "\n",
    "    # Early stopping condition\n",
    "    if early_stop_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch+1}. No improvement in validation loss for {patience} consecutive epochs.\")\n",
    "        break\n",
    "\n",
    "# Print the best validation loss\n",
    "print(f\"Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "# Plotting the loss\n",
    "plt.plot(train_loss_values, label='Training Loss')\n",
    "plt.plot(val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65f8a2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T10:18:05.334568Z",
     "iopub.status.busy": "2024-10-28T10:18:05.334164Z",
     "iopub.status.idle": "2024-10-28T10:18:05.770414Z",
     "shell.execute_reply": "2024-10-28T10:18:05.768589Z",
     "shell.execute_reply.started": "2024-10-28T10:18:05.334524Z"
    },
    "id": "hmFvjShyns2M",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '3d_AttUnet_slices_spatial_SSIM_MSE.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba8e39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:08:48.486601Z",
     "iopub.status.busy": "2024-10-27T20:08:48.486221Z",
     "iopub.status.idle": "2024-10-27T20:09:24.722283Z",
     "shell.execute_reply": "2024-10-27T20:09:24.721483Z",
     "shell.execute_reply.started": "2024-10-27T20:08:48.486564Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_test = BrainMRIDataset(base_path='/kaggle/input/openbhb/val_quasiraw', volume_list=test_volumes)\n",
    "\n",
    "Slices_test = SliceDatasetFromList(dataset_test)\n",
    "\n",
    "test_loader = DataLoader(Slices_test, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94afed2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:10:34.034495Z",
     "iopub.status.busy": "2024-10-27T20:10:34.033779Z",
     "iopub.status.idle": "2024-10-27T20:10:58.877490Z",
     "shell.execute_reply": "2024-10-27T20:10:58.876574Z",
     "shell.execute_reply.started": "2024-10-27T20:10:34.034453Z"
    },
    "id": "SaO0rXPhnd8A",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Test loop\n",
    "test_loss = 0.0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations for testing\n",
    "    for imgs, targets in tqdm(test_loader, desc='Testing'):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)  # Move data to GPU\n",
    "        outputs = model(imgs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute test loss\n",
    "        test_loss += loss.item()  # Accumulate test loss\n",
    "\n",
    "        # Visualize the input and reconstructed volume at depth 8\n",
    "        depth = 8\n",
    "        fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "        # Original Volume (Target)\n",
    "        ax[0].imshow(targets[0, 0, depth, :, :].cpu().numpy(), cmap='gray')\n",
    "        ax[0].set_title('Original Volume (Target)')\n",
    "        ax[0].axis('off')\n",
    "\n",
    "        # Reconstructed Volume (Output)\n",
    "        ax[1].imshow(outputs[0, 0, depth, :, :].cpu().numpy(), cmap='gray')\n",
    "        ax[1].set_title('Reconstructed Volume (Output)')\n",
    "        ax[1].axis('off')\n",
    "\n",
    "        plt.show()  # Display the plot\n",
    "\n",
    "# Average test loss\n",
    "test_loss /= len(test_loader)\n",
    "print(f\"Average Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6316af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T20:13:56.801939Z",
     "iopub.status.busy": "2024-10-27T20:13:56.801544Z",
     "iopub.status.idle": "2024-10-27T20:14:07.965601Z",
     "shell.execute_reply": "2024-10-27T20:14:07.964694Z",
     "shell.execute_reply.started": "2024-10-27T20:13:56.801901Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "# Define the root directory where all subject subdirectories are stored\n",
    "data_dir = '/kaggle/input/brats20-dataset-training-validation/BraTS2020_TrainingData/MICCAI_BraTS2020_TrainingData/'\n",
    "\n",
    "# Define functions\n",
    "def load_nifti_file(file_path):\n",
    "    return nib.load(file_path).get_fdata()\n",
    "\n",
    "def crop(mri_volume, crop_size=(160, 130, 170)):\n",
    "    d, h, w = mri_volume.shape\n",
    "    new_d, new_h, new_w = crop_size\n",
    "    start_d = (d - new_d) // 2\n",
    "    start_h = (h - new_h) // 2\n",
    "    start_w = (w - new_w) // 2\n",
    "    return mri_volume[:, start_h:start_h+new_h, start_w:start_w+new_w]\n",
    "\n",
    "def resize_volume(mri_volume):\n",
    "    mri_tensor = torch.tensor(mri_volume, dtype=torch.float32)\n",
    "    mri_resized = F.interpolate(\n",
    "        mri_tensor.unsqueeze(0).unsqueeze(0),  # Add batch and channel dimensions\n",
    "        size=(mri_tensor.shape[0], 128, 128),  # Only resize height and width\n",
    "        mode='trilinear',\n",
    "        align_corners=False\n",
    "    )\n",
    "    return mri_resized\n",
    "\n",
    "def get_slices(mri_volume):\n",
    "    slices = []\n",
    "    for i in range(8):\n",
    "        start = i * 16\n",
    "        end = start + 16\n",
    "        slice_chunk = mri_volume[:, :, start:end, :, :]\n",
    "        slices.append(slice_chunk)\n",
    "    return slices\n",
    "\n",
    "# Metric function for residual map evaluation\n",
    "def calculate_residuals(true, predicted):\n",
    "    return torch.abs(true - predicted)\n",
    "\n",
    "# Dice score calculation function\n",
    "def dice_score(true, predicted, threshold=0.5):\n",
    "    true = true.cpu().numpy().flatten()\n",
    "    predicted = (predicted.cpu().numpy() > threshold).astype(int).flatten()\n",
    "    intersection = (true * predicted).sum()\n",
    "    return (2. * intersection) / (true.sum() + predicted.sum())\n",
    "\n",
    "# Loop through all subject subdirectories\n",
    "subjects = [os.path.join(data_dir, subj) for subj in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, subj))]\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "subject_count = 10\n",
    "curr_count = 0\n",
    "\n",
    "for subject_dir in subjects:\n",
    "    if curr_count < subject_count:\n",
    "        # Construct paths for the T2-weighted MRI and segmentation files\n",
    "        t2_path = os.path.join(subject_dir, [f for f in os.listdir(subject_dir) if '_t2.nii' in f][0])\n",
    "        seg_path = t2_path.replace('_t2.nii', '_seg.nii')\n",
    "\n",
    "        # Load the MRI and segmentation volumes\n",
    "        t2_vol = load_nifti_file(t2_path)\n",
    "        seg_vol = load_nifti_file(seg_path)\n",
    "        \n",
    "\n",
    "\n",
    "        # Preprocess the T2 MRI volume\n",
    "        transpose_vol = np.transpose(t2_vol, (2, 0, 1))  # Reorient the volume\n",
    "        cropped_vol = crop(transpose_vol)  # Crop\n",
    "        resized_vol = resize_volume(cropped_vol)  # Resize\n",
    "\n",
    "        # Preprocess the segmentation volume in the same way\n",
    "        transpose_seg_vol = np.transpose(seg_vol, (2, 0, 1))  # Reorient the segmentation\n",
    "        cropped_seg_vol = crop(transpose_seg_vol)  # Crop\n",
    "        resized_seg_vol = resize_volume(cropped_seg_vol)  # Resize the segmentation to match the MRI\n",
    "\n",
    "        # Split the volume into slices (both MRI and segmentation)\n",
    "        slices_vol = get_slices(resized_vol)\n",
    "        slices_seg = get_slices(resized_seg_vol)\n",
    "\n",
    "        # Perform inference using the model\n",
    "        with torch.no_grad():\n",
    "            tensor_vol_slice = slices_vol[4].to(device)  # Taking the 5th chunk as an example (index 4)\n",
    "            output = model(tensor_vol_slice)\n",
    "\n",
    "            # Calculate the residuals\n",
    "            residuals = calculate_residuals(tensor_vol_slice, output)\n",
    "\n",
    "            # Extract the corresponding segmentation slice\n",
    "            tensor_seg_slice = slices_seg[4].to(device)\n",
    "\n",
    "            # Calculate Dice score as a metric\n",
    "            dice = dice_score(tensor_seg_slice, residuals)\n",
    "\n",
    "        # Plot example slices\n",
    "        depth = 8  # Slice depth to visualize\n",
    "        fig, ax = plt.subplots(1, 4, figsize=(24, 6))\n",
    "\n",
    "        # Plot the original MRI volume slice\n",
    "        ax[0].imshow(tensor_vol_slice[0, 0, depth, :, :].cpu().numpy(), cmap='gray')\n",
    "        ax[0].set_title('Original Volume')\n",
    "\n",
    "        # Plot the reconstructed volume slice\n",
    "        ax[1].imshow(output[0, 0, depth, :, :].cpu().numpy(), cmap='gray')\n",
    "        ax[1].set_title('Reconstructed Volume')\n",
    "\n",
    "        # Plot the residual map slice\n",
    "        ax[2].imshow(residuals[0, 0, depth, :, :].cpu().numpy(), cmap='hot')\n",
    "        ax[2].set_title(f'Residuals (Dice: {dice:.4f})')\n",
    "\n",
    "        # Plot the corresponding segmentation map slice\n",
    "        ax[3].imshow(tensor_seg_slice[0, 0, depth, :, :].cpu().numpy(), cmap='gray')\n",
    "        ax[3].set_title('Segmentation Map')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Processed {subject_dir}: Dice Score: {dice:.4f}\")\n",
    "    \n",
    "    curr_count += 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 751906,
     "sourceId": 1299795,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5844160,
     "sourceId": 9584118,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5962931,
     "sourceId": 9741594,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 250.71344,
   "end_time": "2024-11-02T06:25:15.488119",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-11-02T06:21:04.774679",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
